# llm-quantization
LLM quantization is the process of reducing the precision of a large language modelâ€™s weights (e.g., from 32-bit to 8-bit) to optimize memory usage and computational efficiency while maintaining performance.
