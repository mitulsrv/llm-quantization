{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34bea9f0-928a-4c06-b124-cd13389b1b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19645b33-f290-4f58-a4c6-b8af719af235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3615, 0.8084, 0.0847, 0.3425, 0.2880, 0.9754, 0.2842, 0.6910, 0.8312,\n",
      "        0.0721, 0.0167, 0.0331, 0.0136, 0.3273, 0.0077, 0.4918, 0.2129, 0.1961,\n",
      "        0.4500, 0.2020, 0.1852, 0.9319, 0.7452, 0.5939, 0.6367, 0.4295, 0.1187,\n",
      "        0.2485, 0.7731, 0.7379, 0.0569, 0.6000, 0.3470, 0.5869, 0.1001, 0.2041,\n",
      "        0.1564, 0.2586, 0.1482, 0.4421, 0.7338, 0.1332, 0.2049, 0.7458, 0.6129,\n",
      "        0.3862, 0.6282, 0.6222, 0.4407, 0.4792, 0.5492, 0.2975, 0.3656, 0.3497,\n",
      "        0.0582, 0.5363, 0.5675, 0.3777, 0.7766, 0.2209, 0.8537, 0.7276, 0.9076,\n",
      "        0.6764, 0.7823, 0.8843, 0.6332, 0.0980, 0.1483, 0.9160, 0.9865, 0.0744,\n",
      "        0.5226, 0.1962, 0.2830, 0.9156, 0.0244, 0.6859, 0.1468, 0.8692, 0.9059,\n",
      "        0.5739, 0.5554, 0.9353, 0.6970, 0.8855, 0.1646, 0.8709, 0.8433, 0.8853,\n",
      "        0.0920, 0.4631, 0.9860, 0.0833, 0.9052, 0.9459, 0.0996, 0.0013, 0.8619,\n",
      "        0.8281, 0.2770, 0.0483, 0.1396, 0.8745, 0.1682, 0.6226, 0.8038, 0.5462,\n",
      "        0.4253, 0.2071, 0.0231, 0.6860, 0.6492, 0.9877, 0.9475, 0.0885, 0.5552,\n",
      "        0.3975, 0.1178, 0.1551, 0.8428, 0.6082, 0.5830, 0.6128, 0.4218, 0.8921,\n",
      "        0.0596, 0.3983, 0.8196, 0.5300, 0.0819, 0.1335, 0.5302, 0.1422, 0.6016,\n",
      "        0.6313, 0.8715, 0.2832, 0.9041, 0.0980, 0.6542, 0.4835, 0.4947, 0.4595,\n",
      "        0.0840, 0.4529, 0.4979, 0.6952, 0.4290, 0.8473, 0.0414, 0.2409, 0.2893,\n",
      "        0.6293, 0.7023, 0.0358, 0.8556, 0.0347, 0.1951, 0.4324, 0.8944, 0.8294,\n",
      "        0.8827, 0.3297, 0.8351, 0.5877, 0.1416, 0.5392, 0.7127, 0.3006, 0.6461,\n",
      "        0.9596, 0.3206, 0.8083, 0.8057, 0.2491, 0.4744, 0.9315, 0.5142, 0.5901,\n",
      "        0.9323, 0.8610, 0.7768, 0.6531, 0.7034, 0.5615, 0.9084, 0.8694, 0.5226,\n",
      "        0.5758, 0.0041, 0.6165, 0.2340, 0.0945, 0.4767, 0.4727, 0.5595, 0.5221,\n",
      "        0.6911, 0.0961, 0.3745, 0.0215, 0.6680, 0.2650, 0.4311, 0.8027, 0.9129,\n",
      "        0.7409, 0.2617, 0.7907, 0.9922, 0.6447, 0.8026, 0.3963, 0.1593, 0.1763,\n",
      "        0.8266, 0.3814, 0.3310, 0.6639, 0.0779, 0.5051, 0.3702, 0.0790, 0.9088,\n",
      "        0.7799, 0.8048, 0.0348, 0.9821, 0.5422, 0.3180, 0.4620, 0.3124, 0.2089,\n",
      "        0.2276, 0.6489, 0.3690, 0.0713, 0.4620, 0.5887, 0.9392, 0.5332, 0.3293,\n",
      "        0.5559, 0.5189, 0.7124, 0.6319, 0.3662, 0.6609, 0.3656, 0.3485, 0.8915,\n",
      "        0.6298, 0.8691, 0.4592, 0.0567, 0.8749, 0.9364, 0.7934, 0.2944, 0.7333,\n",
      "        0.3800, 0.2689, 0.1674, 0.0387, 0.8888, 0.5931, 0.4667, 0.4514, 0.8649,\n",
      "        0.7548, 0.9961, 0.5041, 0.2175, 0.3058, 0.8316, 0.2442, 0.8008, 0.7043,\n",
      "        0.6057, 0.3651, 0.4566, 0.0251, 0.7996, 0.5992, 0.2850, 0.5568, 0.1821,\n",
      "        0.1740, 0.0536, 0.5649, 0.3995, 0.5803, 0.0388, 0.8992, 0.4638, 0.3307,\n",
      "        0.0334, 0.8888, 0.6769, 0.9729, 0.2670, 0.5641, 0.6265, 0.8884, 0.8126,\n",
      "        0.6114, 0.0753, 0.3162, 0.1458, 0.3513, 0.4052, 0.6548, 0.3791, 0.5911,\n",
      "        0.6057, 0.0214, 0.7031, 0.5769, 0.2120, 0.6611, 0.0560, 0.4522, 0.8835,\n",
      "        0.6296, 0.5051, 0.7586, 0.1398, 0.2489, 0.7577, 0.6714, 0.5264, 0.6737,\n",
      "        0.5338, 0.0221, 0.7601, 0.5613, 0.3490, 0.1119, 0.2209, 0.5026, 0.4946,\n",
      "        0.4971, 0.2423, 0.7920, 0.6166, 0.8434, 0.3935, 0.3514, 0.3640, 0.2869,\n",
      "        0.2640, 0.9421, 0.2319, 0.0438, 0.0582, 0.4577, 0.9607, 0.5512, 0.0915,\n",
      "        0.6654, 0.1493, 0.3144, 0.0704, 0.5140, 0.9672, 0.7659, 0.0474, 0.0086,\n",
      "        0.2766, 0.2131, 0.6347, 0.4803, 0.8457, 0.9513, 0.8589, 0.9105, 0.6734,\n",
      "        0.9348, 0.9725, 0.1730, 0.5232, 0.7177, 0.7136, 0.7034, 0.2528, 0.2433,\n",
      "        0.2767, 0.0666, 0.5904, 0.4070, 0.4520, 0.9073, 0.5462, 0.2653, 0.0719,\n",
      "        0.6391, 0.3513, 0.7002, 0.1810, 0.1046, 0.0689, 0.2596, 0.9138, 0.5659,\n",
      "        0.7823, 0.7436, 0.8795, 0.4450, 0.6354, 0.8360, 0.7311, 0.2686, 0.9295,\n",
      "        0.3146, 0.5551, 0.8849, 0.6587, 0.0556, 0.1579, 0.5358, 0.7376, 0.6023,\n",
      "        0.7434, 0.0540, 0.0162, 0.9540, 0.3369, 0.1811, 0.1429, 0.8755, 0.3212,\n",
      "        0.2106, 0.8316, 0.1664, 0.1090, 0.2734, 0.3617, 0.1895, 0.3449, 0.2292,\n",
      "        0.6435, 0.2961, 0.2511, 0.8913, 0.1011, 0.7626, 0.6983, 0.4616, 0.8665,\n",
      "        0.1118, 0.0141, 0.2677, 0.4380, 0.7640, 0.9917, 0.4133, 0.6519, 0.0555,\n",
      "        0.3660, 0.4581, 0.9027, 0.2548, 0.7401, 0.0683, 0.9108, 0.2418, 0.1063,\n",
      "        0.4238, 0.4155, 0.5533, 0.0641, 0.3849, 0.7209, 0.2900, 0.0736, 0.4153,\n",
      "        0.2891, 0.6748, 0.2676, 0.1251, 0.4556, 0.9235, 0.6305, 0.9260, 0.0074,\n",
      "        0.0671, 0.5643, 0.1759, 0.0796, 0.0986, 0.2841, 0.9727, 0.8433, 0.8058,\n",
      "        0.7635, 0.2620, 0.6362, 0.9103, 0.2174, 0.9817, 0.8330, 0.3188, 0.2100,\n",
      "        0.8009, 0.5904, 0.2482, 0.1535, 0.6429, 0.9398, 0.5695, 0.9757, 0.6011,\n",
      "        0.3095, 0.1417, 0.9286, 0.0561, 0.4358, 0.0808, 0.8755, 0.0190, 0.1765,\n",
      "        0.0236, 0.7711, 0.8271, 0.1504, 0.0405, 0.2576, 0.4023, 0.8141, 0.7244,\n",
      "        0.5693, 0.6023, 0.9170, 0.9396, 0.6176, 0.0865, 0.9849, 0.9140, 0.0615,\n",
      "        0.4948, 0.3969, 0.4735, 0.4050, 0.3448, 0.3212, 0.6523, 0.2731, 0.5932,\n",
      "        0.5297, 0.0761, 0.8464, 0.7086, 0.2964, 0.7089, 0.5990, 0.0577, 0.0868,\n",
      "        0.1708, 0.3280, 0.7608, 0.2793, 0.2209, 0.4318, 0.9230, 0.1275, 0.7730,\n",
      "        0.5340, 0.0197, 0.5045, 0.8368, 0.1909, 0.8814, 0.9884, 0.2524, 0.3380,\n",
      "        0.6162, 0.1160, 0.6317, 0.8871, 0.4856, 0.6607, 0.3085, 0.3968, 0.8177,\n",
      "        0.8463, 0.6419, 0.9365, 0.9170, 0.1120, 0.7342, 0.1371, 0.0367, 0.5143,\n",
      "        0.5917, 0.8770, 0.4622, 0.4460, 0.7756, 0.5003, 0.6129, 0.4586, 0.8948,\n",
      "        0.3389, 0.0217, 0.6035, 0.9426, 0.1823, 0.3102, 0.3493, 0.6206, 0.3728,\n",
      "        0.7666, 0.5653, 0.4023, 0.9963, 0.9937, 0.9714, 0.5070, 0.4333, 0.8372,\n",
      "        0.8614, 0.7038, 0.9552, 0.8735, 0.6459, 0.3619, 0.1650, 0.6440, 0.8617,\n",
      "        0.8129, 0.8883, 0.4157, 0.5511, 0.4685, 0.6449, 0.8997, 0.2755, 0.2019,\n",
      "        0.1529, 0.6222, 0.0987, 0.1474, 0.6269, 0.4782, 0.9220, 0.4940, 0.2008,\n",
      "        0.6335, 0.2441, 0.5790, 0.4477, 0.9852, 0.2927, 0.3593, 0.0235, 0.4768,\n",
      "        0.0630, 0.9215, 0.6930, 0.7783, 0.4102, 0.9202, 0.2809, 0.6743, 0.6330,\n",
      "        0.1668, 0.6277, 0.1962, 0.9616, 0.7938, 0.5021, 0.9936, 0.4451, 0.5762,\n",
      "        0.1148, 0.9223, 0.6835, 0.3907, 0.1654, 0.1289, 0.3000, 0.2181, 0.2865,\n",
      "        0.6874, 0.5299, 0.4536, 0.9580, 0.7859, 0.4186, 0.5900, 0.1469, 0.5999,\n",
      "        0.7522, 0.0629, 0.9986, 0.4619, 0.6076, 0.4228, 0.6496, 0.6273, 0.1957,\n",
      "        0.9287, 0.7757, 0.6544, 0.2309, 0.0662, 0.6942, 0.1448, 0.9900, 0.8579,\n",
      "        0.4761, 0.2297, 0.3181, 0.7152, 0.3956, 0.4008, 0.0436, 0.3167, 0.9687,\n",
      "        0.4563, 0.2178, 0.9782, 0.2964, 0.0485, 0.3919, 0.8090, 0.3717, 0.2701,\n",
      "        0.3559, 0.2373, 0.8668, 0.3226, 0.4556, 0.4074, 0.6789, 0.4051, 0.4202,\n",
      "        0.3995, 0.6431, 0.5817, 0.5767, 0.6260, 0.7570, 0.6782, 0.6785, 0.0579,\n",
      "        0.5193, 0.3521, 0.1007, 0.8380, 0.1700, 0.6017, 0.1372, 0.0746, 0.6520,\n",
      "        0.7124, 0.8845, 0.4613, 0.1333, 0.6506, 0.3979, 0.3242, 0.2544, 0.1980,\n",
      "        0.3898, 0.0706, 0.2693, 0.9583, 0.5757, 0.9543, 0.5214, 0.0622, 0.4694,\n",
      "        0.4442, 0.1826, 0.8855, 0.7228, 0.4863, 0.6796, 0.1232, 0.6001, 0.9666,\n",
      "        0.5165, 0.0503, 0.2059, 0.4458, 0.3861, 0.2549, 0.9200, 0.6169, 0.4247,\n",
      "        0.4568, 0.8945, 0.4991, 0.3879, 0.8704, 0.4058, 0.6190, 0.8493, 0.7340,\n",
      "        0.4879, 0.6854, 0.6075, 0.8748, 0.0171, 0.4207, 0.7753, 0.2110, 0.9367,\n",
      "        0.1315, 0.7911, 0.8804, 0.6693, 0.1437, 0.6277, 0.2091, 0.4578, 0.2150,\n",
      "        0.3192, 0.5100, 0.5581, 0.0954, 0.4864, 0.5214, 0.2772, 0.9519, 0.8013,\n",
      "        0.0748, 0.3877, 0.8689, 0.1371, 0.4370, 0.0366, 0.3979, 0.8648, 0.4621,\n",
      "        0.6748, 0.4545, 0.5842, 0.2585, 0.3472, 0.6285, 0.7225, 0.6190, 0.6460,\n",
      "        0.5087, 0.6710, 0.7693, 0.1648, 0.9532, 0.8287, 0.6631, 0.4695, 0.8880,\n",
      "        0.7344, 0.1564, 0.0372, 0.5929, 0.7864, 0.4588, 0.5734, 0.5680, 0.7866,\n",
      "        0.1608, 0.8531, 0.6341, 0.6455, 0.6219, 0.5157, 0.5335, 0.9170, 0.1281,\n",
      "        0.1971, 0.4842, 0.7850, 0.4196, 0.4577, 0.4444, 0.0249, 0.9464, 0.9350,\n",
      "        0.3849, 0.1420, 0.6143, 0.6368, 0.4745, 0.0033, 0.3819, 0.9092, 0.4999,\n",
      "        0.7714, 0.3387, 0.3211, 0.4303, 0.4139, 0.8332, 0.4819, 0.0421, 0.4924,\n",
      "        0.8321, 0.5148, 0.2114, 0.2324, 0.8666, 0.5960, 0.1489, 0.7388, 0.3824,\n",
      "        0.8912, 0.9726, 0.9365, 0.9362, 0.0077, 0.1326, 0.9829, 0.7744, 0.7721,\n",
      "        0.8759, 0.6994, 0.6377, 0.9869, 0.0938, 0.9417, 0.2249, 0.0678, 0.1658,\n",
      "        0.4243, 0.6054, 0.0697, 0.2000, 0.1914, 0.0424, 0.6756, 0.9595, 0.4514,\n",
      "        0.2379, 0.1464, 0.5996, 0.3341, 0.2361, 0.2596, 0.1096, 0.3653, 0.9900,\n",
      "        0.2777, 0.2580, 0.5486, 0.2777, 0.7290, 0.1107, 0.8268, 0.0535, 0.0055,\n",
      "        0.2127, 0.5838, 0.5843, 0.2607, 0.1778, 0.4238, 0.2531, 0.3995, 0.4653,\n",
      "        0.9717, 0.0191, 0.6147, 0.4540, 0.7178, 0.1857, 0.9436, 0.6573, 0.7853,\n",
      "        0.7607, 0.4382, 0.7630, 0.0236, 0.5596, 0.7402, 0.1065, 0.6016, 0.7798,\n",
      "        0.0157, 0.9293, 0.7250, 0.8325, 0.2950, 0.8779, 0.1138, 0.5939, 0.6283,\n",
      "        0.8097, 0.8135, 0.9828, 0.1061, 0.0667, 0.2314, 0.1909, 0.5461, 0.8435,\n",
      "        0.0373])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# random pytorch tensor: float32, size=1000\n",
    "tensor_fp32 = torch.rand(1000, dtype = torch.float32)\n",
    "print(tensor_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4397ab47-3677-47f1-b6ba-ce7d56730875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3613, 0.8086, 0.0850, 0.3418, 0.2871, 0.9766, 0.2832, 0.6914, 0.8320,\n",
      "        0.0723, 0.0167, 0.0332, 0.0136, 0.3281, 0.0077, 0.4922, 0.2129, 0.1963,\n",
      "        0.4492, 0.2021, 0.1855, 0.9336, 0.7461, 0.5938, 0.6367, 0.4297, 0.1187,\n",
      "        0.2480, 0.7734, 0.7383, 0.0569, 0.6016, 0.3477, 0.5859, 0.1001, 0.2041,\n",
      "        0.1562, 0.2578, 0.1484, 0.4414, 0.7344, 0.1328, 0.2051, 0.7461, 0.6133,\n",
      "        0.3867, 0.6289, 0.6211, 0.4414, 0.4785, 0.5508, 0.2969, 0.3652, 0.3496,\n",
      "        0.0581, 0.5352, 0.5664, 0.3770, 0.7773, 0.2207, 0.8555, 0.7266, 0.9062,\n",
      "        0.6758, 0.7812, 0.8828, 0.6328, 0.0981, 0.1484, 0.9180, 0.9883, 0.0742,\n",
      "        0.5234, 0.1963, 0.2832, 0.9141, 0.0244, 0.6875, 0.1465, 0.8711, 0.9062,\n",
      "        0.5742, 0.5547, 0.9336, 0.6953, 0.8867, 0.1650, 0.8711, 0.8438, 0.8867,\n",
      "        0.0918, 0.4629, 0.9844, 0.0835, 0.9062, 0.9453, 0.0996, 0.0013, 0.8633,\n",
      "        0.8281, 0.2773, 0.0483, 0.1396, 0.8750, 0.1680, 0.6211, 0.8047, 0.5469,\n",
      "        0.4258, 0.2070, 0.0231, 0.6875, 0.6484, 0.9883, 0.9492, 0.0884, 0.5547,\n",
      "        0.3984, 0.1177, 0.1553, 0.8438, 0.6094, 0.5820, 0.6133, 0.4219, 0.8906,\n",
      "        0.0596, 0.3984, 0.8203, 0.5312, 0.0820, 0.1338, 0.5312, 0.1426, 0.6016,\n",
      "        0.6328, 0.8711, 0.2832, 0.9023, 0.0981, 0.6523, 0.4844, 0.4941, 0.4590,\n",
      "        0.0840, 0.4531, 0.4980, 0.6953, 0.4297, 0.8477, 0.0413, 0.2412, 0.2891,\n",
      "        0.6289, 0.7031, 0.0356, 0.8555, 0.0347, 0.1953, 0.4316, 0.8945, 0.8281,\n",
      "        0.8828, 0.3301, 0.8359, 0.5859, 0.1416, 0.5391, 0.7109, 0.3008, 0.6445,\n",
      "        0.9609, 0.3203, 0.8086, 0.8047, 0.2490, 0.4746, 0.9297, 0.5156, 0.5898,\n",
      "        0.9336, 0.8594, 0.7773, 0.6523, 0.7031, 0.5625, 0.9102, 0.8711, 0.5234,\n",
      "        0.5742, 0.0041, 0.6172, 0.2344, 0.0947, 0.4766, 0.4727, 0.5586, 0.5234,\n",
      "        0.6914, 0.0962, 0.3750, 0.0215, 0.6680, 0.2656, 0.4316, 0.8047, 0.9141,\n",
      "        0.7422, 0.2617, 0.7891, 0.9922, 0.6445, 0.8008, 0.3965, 0.1592, 0.1758,\n",
      "        0.8281, 0.3809, 0.3301, 0.6641, 0.0781, 0.5039, 0.3711, 0.0791, 0.9102,\n",
      "        0.7812, 0.8047, 0.0349, 0.9805, 0.5430, 0.3184, 0.4629, 0.3125, 0.2090,\n",
      "        0.2275, 0.6484, 0.3691, 0.0713, 0.4629, 0.5898, 0.9375, 0.5352, 0.3301,\n",
      "        0.5547, 0.5195, 0.7109, 0.6328, 0.3652, 0.6602, 0.3652, 0.3477, 0.8906,\n",
      "        0.6289, 0.8672, 0.4590, 0.0566, 0.8750, 0.9375, 0.7930, 0.2949, 0.7344,\n",
      "        0.3809, 0.2695, 0.1670, 0.0388, 0.8906, 0.5938, 0.4668, 0.4512, 0.8633,\n",
      "        0.7539, 0.9961, 0.5039, 0.2178, 0.3066, 0.8320, 0.2441, 0.8008, 0.7031,\n",
      "        0.6055, 0.3652, 0.4570, 0.0250, 0.8008, 0.5977, 0.2852, 0.5586, 0.1816,\n",
      "        0.1738, 0.0535, 0.5664, 0.4004, 0.5820, 0.0388, 0.8984, 0.4629, 0.3301,\n",
      "        0.0334, 0.8906, 0.6758, 0.9727, 0.2676, 0.5625, 0.6250, 0.8867, 0.8125,\n",
      "        0.6133, 0.0752, 0.3164, 0.1455, 0.3516, 0.4043, 0.6562, 0.3789, 0.5898,\n",
      "        0.6055, 0.0214, 0.7031, 0.5781, 0.2119, 0.6602, 0.0559, 0.4531, 0.8828,\n",
      "        0.6289, 0.5039, 0.7578, 0.1396, 0.2490, 0.7578, 0.6719, 0.5273, 0.6719,\n",
      "        0.5352, 0.0221, 0.7617, 0.5625, 0.3496, 0.1118, 0.2207, 0.5039, 0.4941,\n",
      "        0.4961, 0.2422, 0.7930, 0.6172, 0.8438, 0.3926, 0.3516, 0.3633, 0.2871,\n",
      "        0.2637, 0.9414, 0.2314, 0.0437, 0.0581, 0.4570, 0.9609, 0.5508, 0.0913,\n",
      "        0.6641, 0.1494, 0.3145, 0.0703, 0.5156, 0.9688, 0.7656, 0.0474, 0.0086,\n",
      "        0.2773, 0.2129, 0.6328, 0.4805, 0.8438, 0.9531, 0.8594, 0.9102, 0.6719,\n",
      "        0.9336, 0.9727, 0.1729, 0.5234, 0.7188, 0.7148, 0.7031, 0.2520, 0.2432,\n",
      "        0.2773, 0.0664, 0.5898, 0.4062, 0.4512, 0.9062, 0.5469, 0.2656, 0.0718,\n",
      "        0.6406, 0.3516, 0.6992, 0.1807, 0.1045, 0.0688, 0.2598, 0.9141, 0.5664,\n",
      "        0.7812, 0.7422, 0.8789, 0.4453, 0.6367, 0.8359, 0.7305, 0.2695, 0.9297,\n",
      "        0.3145, 0.5547, 0.8867, 0.6602, 0.0557, 0.1582, 0.5352, 0.7383, 0.6016,\n",
      "        0.7422, 0.0540, 0.0162, 0.9531, 0.3379, 0.1807, 0.1426, 0.8750, 0.3203,\n",
      "        0.2109, 0.8320, 0.1660, 0.1089, 0.2734, 0.3613, 0.1895, 0.3457, 0.2295,\n",
      "        0.6445, 0.2969, 0.2520, 0.8906, 0.1011, 0.7617, 0.6992, 0.4609, 0.8672,\n",
      "        0.1118, 0.0142, 0.2676, 0.4375, 0.7656, 0.9922, 0.4141, 0.6523, 0.0554,\n",
      "        0.3652, 0.4590, 0.9023, 0.2539, 0.7383, 0.0684, 0.9102, 0.2422, 0.1064,\n",
      "        0.4238, 0.4160, 0.5547, 0.0640, 0.3848, 0.7227, 0.2891, 0.0737, 0.4160,\n",
      "        0.2891, 0.6758, 0.2676, 0.1250, 0.4551, 0.9219, 0.6289, 0.9258, 0.0074,\n",
      "        0.0669, 0.5625, 0.1758, 0.0796, 0.0986, 0.2832, 0.9727, 0.8438, 0.8047,\n",
      "        0.7617, 0.2617, 0.6367, 0.9102, 0.2178, 0.9805, 0.8320, 0.3184, 0.2100,\n",
      "        0.8008, 0.5898, 0.2480, 0.1533, 0.6445, 0.9414, 0.5703, 0.9766, 0.6016,\n",
      "        0.3086, 0.1416, 0.9297, 0.0562, 0.4355, 0.0806, 0.8750, 0.0190, 0.1768,\n",
      "        0.0237, 0.7695, 0.8281, 0.1504, 0.0405, 0.2578, 0.4023, 0.8125, 0.7227,\n",
      "        0.5703, 0.6016, 0.9180, 0.9414, 0.6172, 0.0864, 0.9844, 0.9141, 0.0615,\n",
      "        0.4941, 0.3965, 0.4727, 0.4043, 0.3457, 0.3203, 0.6523, 0.2734, 0.5938,\n",
      "        0.5312, 0.0762, 0.8477, 0.7070, 0.2969, 0.7070, 0.5977, 0.0576, 0.0869,\n",
      "        0.1709, 0.3281, 0.7617, 0.2793, 0.2207, 0.4316, 0.9219, 0.1279, 0.7734,\n",
      "        0.5352, 0.0198, 0.5039, 0.8359, 0.1904, 0.8828, 0.9883, 0.2520, 0.3379,\n",
      "        0.6172, 0.1162, 0.6328, 0.8867, 0.4863, 0.6602, 0.3086, 0.3965, 0.8164,\n",
      "        0.8477, 0.6406, 0.9375, 0.9180, 0.1118, 0.7344, 0.1367, 0.0366, 0.5156,\n",
      "        0.5898, 0.8789, 0.4629, 0.4453, 0.7773, 0.5000, 0.6133, 0.4590, 0.8945,\n",
      "        0.3398, 0.0217, 0.6016, 0.9414, 0.1826, 0.3105, 0.3496, 0.6211, 0.3730,\n",
      "        0.7656, 0.5664, 0.4023, 0.9961, 0.9922, 0.9727, 0.5078, 0.4336, 0.8359,\n",
      "        0.8633, 0.7031, 0.9570, 0.8750, 0.6445, 0.3613, 0.1650, 0.6445, 0.8633,\n",
      "        0.8125, 0.8867, 0.4160, 0.5508, 0.4688, 0.6445, 0.8984, 0.2754, 0.2021,\n",
      "        0.1533, 0.6211, 0.0986, 0.1475, 0.6250, 0.4785, 0.9219, 0.4941, 0.2012,\n",
      "        0.6328, 0.2441, 0.5781, 0.4473, 0.9844, 0.2930, 0.3594, 0.0236, 0.4766,\n",
      "        0.0630, 0.9219, 0.6914, 0.7773, 0.4102, 0.9219, 0.2812, 0.6758, 0.6328,\n",
      "        0.1670, 0.6289, 0.1963, 0.9609, 0.7930, 0.5039, 0.9922, 0.4453, 0.5781,\n",
      "        0.1147, 0.9219, 0.6836, 0.3906, 0.1650, 0.1289, 0.3008, 0.2178, 0.2871,\n",
      "        0.6875, 0.5312, 0.4531, 0.9570, 0.7852, 0.4180, 0.5898, 0.1465, 0.6016,\n",
      "        0.7539, 0.0630, 1.0000, 0.4629, 0.6094, 0.4219, 0.6484, 0.6289, 0.1953,\n",
      "        0.9297, 0.7773, 0.6562, 0.2305, 0.0664, 0.6953, 0.1445, 0.9883, 0.8594,\n",
      "        0.4766, 0.2295, 0.3184, 0.7148, 0.3965, 0.4004, 0.0437, 0.3164, 0.9688,\n",
      "        0.4570, 0.2178, 0.9766, 0.2969, 0.0486, 0.3926, 0.8086, 0.3711, 0.2695,\n",
      "        0.3555, 0.2373, 0.8672, 0.3223, 0.4551, 0.4082, 0.6797, 0.4043, 0.4199,\n",
      "        0.4004, 0.6445, 0.5820, 0.5781, 0.6250, 0.7578, 0.6797, 0.6797, 0.0579,\n",
      "        0.5195, 0.3516, 0.1006, 0.8398, 0.1699, 0.6016, 0.1377, 0.0747, 0.6523,\n",
      "        0.7109, 0.8828, 0.4609, 0.1328, 0.6523, 0.3984, 0.3242, 0.2539, 0.1982,\n",
      "        0.3906, 0.0708, 0.2695, 0.9570, 0.5742, 0.9531, 0.5195, 0.0623, 0.4688,\n",
      "        0.4434, 0.1826, 0.8867, 0.7227, 0.4863, 0.6797, 0.1230, 0.6016, 0.9648,\n",
      "        0.5156, 0.0503, 0.2061, 0.4453, 0.3867, 0.2539, 0.9219, 0.6172, 0.4238,\n",
      "        0.4570, 0.8945, 0.5000, 0.3887, 0.8711, 0.4062, 0.6172, 0.8477, 0.7344,\n",
      "        0.4883, 0.6836, 0.6094, 0.8750, 0.0171, 0.4199, 0.7734, 0.2109, 0.9375,\n",
      "        0.1318, 0.7930, 0.8789, 0.6680, 0.1436, 0.6289, 0.2090, 0.4570, 0.2148,\n",
      "        0.3184, 0.5117, 0.5586, 0.0952, 0.4863, 0.5195, 0.2773, 0.9531, 0.8008,\n",
      "        0.0747, 0.3867, 0.8672, 0.1367, 0.4375, 0.0366, 0.3984, 0.8633, 0.4629,\n",
      "        0.6758, 0.4551, 0.5859, 0.2578, 0.3477, 0.6289, 0.7227, 0.6172, 0.6445,\n",
      "        0.5078, 0.6719, 0.7695, 0.1650, 0.9531, 0.8281, 0.6641, 0.4688, 0.8867,\n",
      "        0.7344, 0.1562, 0.0371, 0.5938, 0.7852, 0.4590, 0.5742, 0.5664, 0.7852,\n",
      "        0.1611, 0.8516, 0.6328, 0.6445, 0.6211, 0.5156, 0.5352, 0.9180, 0.1279,\n",
      "        0.1973, 0.4844, 0.7852, 0.4199, 0.4570, 0.4453, 0.0249, 0.9453, 0.9336,\n",
      "        0.3848, 0.1416, 0.6133, 0.6367, 0.4746, 0.0033, 0.3828, 0.9102, 0.5000,\n",
      "        0.7695, 0.3379, 0.3203, 0.4297, 0.4141, 0.8320, 0.4824, 0.0422, 0.4922,\n",
      "        0.8320, 0.5156, 0.2109, 0.2324, 0.8672, 0.5977, 0.1484, 0.7383, 0.3828,\n",
      "        0.8906, 0.9727, 0.9375, 0.9375, 0.0077, 0.1328, 0.9844, 0.7734, 0.7734,\n",
      "        0.8750, 0.6992, 0.6367, 0.9883, 0.0938, 0.9414, 0.2246, 0.0679, 0.1660,\n",
      "        0.4238, 0.6055, 0.0698, 0.2002, 0.1914, 0.0425, 0.6758, 0.9609, 0.4512,\n",
      "        0.2383, 0.1465, 0.5977, 0.3340, 0.2363, 0.2598, 0.1094, 0.3652, 0.9883,\n",
      "        0.2773, 0.2578, 0.5469, 0.2773, 0.7305, 0.1108, 0.8281, 0.0535, 0.0055,\n",
      "        0.2129, 0.5820, 0.5859, 0.2598, 0.1777, 0.4238, 0.2539, 0.4004, 0.4648,\n",
      "        0.9727, 0.0192, 0.6133, 0.4531, 0.7188, 0.1855, 0.9453, 0.6562, 0.7852,\n",
      "        0.7617, 0.4375, 0.7617, 0.0236, 0.5586, 0.7422, 0.1064, 0.6016, 0.7812,\n",
      "        0.0157, 0.9297, 0.7266, 0.8320, 0.2949, 0.8789, 0.1138, 0.5938, 0.6289,\n",
      "        0.8086, 0.8125, 0.9844, 0.1060, 0.0669, 0.2314, 0.1904, 0.5469, 0.8438,\n",
      "        0.0374], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "tensor_fp32_to_fp16 = tensor_fp32.to(dtype=torch.bfloat16)\n",
    "print(tensor_fp32_to_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0798e6db-72f5-49b7-b281-9c0c64ece54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(329.7125)\n",
      "tensor(330., dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "m_float32 = torch.dot(tensor_fp32, tensor_fp32)\n",
    "print(m_float32)\n",
    "m_float16 = torch.dot(tensor_fp32_to_fp16, tensor_fp32_to_fp16)\n",
    "print(m_float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a360e952-5c4b-4dc8-87e4-f4caa00761a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfe5d7e7-d9ed-4295-b5b7-ffe98662c42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/anaconda3/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.26.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.3.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02758e46-ecec-483c-8ca5-bdbfec37f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"HF_TOKEN\"\n",
    "model_name = \"EleutherAI/pythia-410m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3b51266-069c-4963-8338-cf1bf1cf1e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXModel(\n",
      "  (embed_in): Embedding(50304, 1024)\n",
      "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x GPTNeoXLayer(\n",
      "      (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attention): GPTNeoXSdpaAttention(\n",
      "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "        (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp): GPTNeoXMLP(\n",
      "        (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (act): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.gpt_neox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dba8668e-fcf6-4ae8-9c60-ff9d7165ac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a man who was a great\n",
      "and powerful king.\n",
      "{'input_ids': tensor([[10758,  2220,   247,   673,    13,   627,   369,   247]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[10758,  2220,   247,   673,    13,   627,   369,   247,   637,   665,\n",
      "           369,   247,  1270,   187,   395,  6422,  6963,    15]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Once upon a time, there was a\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61697ad5-2f1c-4084-ab5f-6cce084960d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0061, -0.0016, -0.0068,  ..., -0.0062,  0.0138,  0.0222],\n",
      "        [ 0.0077,  0.0157, -0.0090,  ...,  0.0013, -0.0132,  0.0109],\n",
      "        [-0.0330,  0.0008,  0.0281,  ...,  0.0026,  0.0456, -0.0077],\n",
      "        ...,\n",
      "        [-0.0105,  0.0091, -0.0137,  ..., -0.0046,  0.0371, -0.0077],\n",
      "        [-0.0063,  0.0035,  0.0147,  ...,  0.0220,  0.0158,  0.0224],\n",
      "        [-0.0299,  0.0129,  0.0208,  ..., -0.0040, -0.0065,  0.0122]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.gpt_neox.layers[0].attention.dense.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71db1201-5c2c-41e5-867b-220be19e5307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: quanto in /opt/anaconda3/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from quanto) (2.5.0)\n",
      "Requirement already satisfied: ninja in /opt/anaconda3/lib/python3.12/site-packages (from quanto) (1.11.1.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from quanto) (1.26.4)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/lib/python3.12/site-packages (from quanto) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.2.0->quanto) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.2.0->quanto) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.2.0->quanto) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.2.0->quanto) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.2.0->quanto) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.2.0->quanto) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.2.0->quanto) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.2.0->quanto) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.2.0->quanto) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9532b9e1-eaa5-4e54-ad7a-7c6084658e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quanto import quantize, freeze, qint8\n",
    "quantize(model, weights=qint8, activations=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f915418-923f-4b31-b0a5-f73797b6f373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 1024)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXSdpaAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): QLinear(in_features=1024, out_features=3072, bias=True)\n",
      "          (dense): QLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): QLinear(in_features=1024, out_features=4096, bias=True)\n",
      "          (dense_4h_to_h): QLinear(in_features=4096, out_features=1024, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): QLinear(in_features=1024, out_features=50304, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a843dd7-56e9-49ff-8777-faa01cf3d7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0061, -0.0016, -0.0068,  ..., -0.0062,  0.0138,  0.0222],\n",
      "        [ 0.0077,  0.0157, -0.0090,  ...,  0.0013, -0.0132,  0.0109],\n",
      "        [-0.0330,  0.0008,  0.0281,  ...,  0.0026,  0.0456, -0.0077],\n",
      "        ...,\n",
      "        [-0.0105,  0.0091, -0.0137,  ..., -0.0046,  0.0371, -0.0077],\n",
      "        [-0.0063,  0.0035,  0.0147,  ...,  0.0220,  0.0158,  0.0224],\n",
      "        [-0.0299,  0.0129,  0.0208,  ..., -0.0040, -0.0065,  0.0122]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.gpt_neox.layers[0].attention.dense.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b70f42d8-4795-4e55-a856-6436f79dce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85dd669a-7416-48d1-863e-501540f66980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QBytesTensor(tensor([[ 12,  -3, -14,  ..., -12,  28,  45],\n",
      "        [ 18,  37, -21,  ...,   3, -31,  26],\n",
      "        [-75,   2,  64,  ...,   6, 104, -18],\n",
      "        ...,\n",
      "        [-25,  22, -33,  ..., -11,  89, -19],\n",
      "        [-14,   8,  33,  ...,  49,  35,  50],\n",
      "        [-56,  24,  39,  ...,  -8, -12,  23]], dtype=torch.int8), scale=tensor([[0.0005],\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        ...,\n",
      "        [0.0004],\n",
      "        [0.0004],\n",
      "        [0.0005]]), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "print(model.gpt_neox.layers[0].attention.dense.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bbf5424f-e51b-4c04-9954-10733c58100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Consider the below context and answer accordingly.\n",
      "    Lions are found in Gir, Gujarat. Those lions are orange and that's why they known as Kesari. Their population is around 1 lacs.\n",
      "\n",
      "    Where Kesari found and who are they?\n",
      "    Kesari is a lion in Gir,\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "    Consider the below context and answer accordingly.\n",
    "    Lions are found in Gir, Gujarat. Those lions are orange and that's why they known as Kesari. Their population is around 1 lacs.\n",
    "\n",
    "    Where Kesari found and who are they?\n",
    "\"\"\"\n",
    "inputs1 = tokenizer(text1, return_tensors=\"pt\")\n",
    "outputs1 = model.generate(**inputs1, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs1[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef05ead-e122-4716-b237-5d8f217f8b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
